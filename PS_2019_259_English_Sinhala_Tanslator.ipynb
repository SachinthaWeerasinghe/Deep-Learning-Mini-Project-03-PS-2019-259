{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SachinthaWeerasinghe/Deep-Learning-Mini-Project-03-PS-2019-259/blob/main/PS_2019_259_English_Sinhala_Tanslator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **English to Sinhala Translattion System using Transformer Neural Network**\n",
        "# **COSC 44323 - Introduction To Deep Learning**\n",
        "\n",
        "**Mini Project No. 03**\n",
        "\n",
        "**Student Number: PS/2019/259**\n",
        "\n",
        "**Name: S.A. WEERASINGHE**"
      ],
      "metadata": {
        "id": "HyUGf-9GJhkS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**IMPORT NECESSARY LIBRARIES**"
      ],
      "metadata": {
        "id": "5RatVuZXJmcg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6tcdLoyvI8x3"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import tensorflow as tf\n",
        "import string\n",
        "import re\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MOUNT GOOGLE DRIVE**"
      ],
      "metadata": {
        "id": "9Km72J4CJxPg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4JyFw233RLGX",
        "outputId": "63d8b528-054c-4554-8614-733901f67a75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**READ THE DATASET**"
      ],
      "metadata": {
        "id": "1kc53tdrRZ42"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_file = \"/content/drive/MyDrive/PS-2019-259-COSC44323-MiniProject03/EnglishSinhalaDataset-PS-2019-259.txt\"\n",
        "with open(text_file) as f:\n",
        "    lines = f.read().split(\"\\n\")[:-1]\n",
        "i = 0\n",
        "for line in lines:\n",
        "  print(line)\n",
        "  i = i + 1\n",
        "  if(i==20):\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWG6l0WhRcMV",
        "outputId": "a3393b97-f4ff-49be-f794-c2c5906df2a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Go.\tයන්න.\n",
            "Hi.\tආයුබෝවන්.\n",
            "Run.\tදුවන්න.\n",
            "Who?\tකව්ද?\n",
            "Wow!\tවාව්!\n",
            "Fire.\tගින්නක්.\n",
            "Help.\tඋදව්.\n",
            "Hide.\tසඟවන්න.\n",
            "Jump.\tපනින්න.\n",
            "Stay.\tරැඳී සිටින්න.\n",
            "Stop.\tනවත්වන්න.\n",
            "Wait.\tඉන්න.\n",
            "Begin.\tආරම්භය.\n",
            "Go on.\tදිගටම යන්න.\n",
            "Hello!\tහෙලෝ!\n",
            "Hurry!\tඉක්මන් කරන්න!\n",
            "I hid.\tමම සැඟවී සිටියෙමි.\n",
            "I ran.\tමම දිව්වා.\n",
            "I try.\tමම උත්සාහ කරමි.\n",
            "I won.\tමම දිනුවා.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for x in range(len(lines)-10,len(lines)):\n",
        "  print(lines[x])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_y5yjiwgRwsu",
        "outputId": "bf9f1d9a-0f62-47f5-8e3b-9ceae6a72deb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No matter how much you try to convince people that chocolate is vanilla, it'll still be chocolate, even though you may manage to convince yourself and a few others that it's vanilla.\tචොක්ලට් යනු වැනිලා බව මිනිසුන්ට ඒත්තු ගැන්වීමට ඔබ කොතරම් උත්සාහ කළත්, එය වැනිලා බව ඔබට සහ තවත් කිහිප දෙනෙකුට ඒත්තු ගැන්විය හැකි වුවද, එය තවමත් චොකලට් වනු ඇත.\n",
            "In 1969, Roger Miller recorded a song called \"You Don't Want My Love.\" Today, this song is better known as \"In the Summer Time.\" It's the first song he wrote and sang that became popular.\t1969 දී රොජර් මිලර් \"ඔබට මගේ ආදරය අවශ්‍ය නැත\" නමින් ගීතයක් පටිගත කළේය. අද මෙම ගීතය වඩාත් ප්‍රචලිත වන්නේ \"ගිම්හානයේ\" යනුවෙනි. එය ඔහු ලියූ සහ ගායනා කළ පළමු ගීතය ජනප්‍රිය විය.\n",
            "A child who is a native speaker usually knows many things about his or her language that a non-native speaker who has been studying for years still does not know and perhaps will never know.\tස්වදේශික කථිකයෙකු වන දරුවෙකු සාමාන්‍යයෙන් ඔහුගේ හෝ ඇයගේ භාෂාව පිළිබඳ බොහෝ දේ දන්නා අතර එය වසර ගණනාවක් තිස්සේ අධ්‍යයනය කරන ස්වදේශික නොවන කථිකයෙකු තවමත් නොදන්නා සහ කිසි විටෙකත් නොදැන සිටිය හැකිය.\n",
            "There are four main causes of alcohol-related death. Injury from car accidents or violence is one. Diseases like cirrhosis of the liver, cancer, heart and blood system diseases are the others.\tමත්පැන් නිසා සිදුවන මරණවලට ප්‍රධාන හේතු හතරක් තිබේ. රිය අනතුරකින් හෝ ප්‍රචණ්ඩත්වයකින් තුවාල වීම එකකි. අක්මාවේ සිරෝසිස්, පිළිකා, හෘද රෝග සහ රුධිර සංසරණ පද්ධතිය වැනි රෝග අනෙක් ඒවා වේ.\n",
            "There are mothers and fathers who will lie awake after the children fall asleep and wonder how they'll make the mortgage, or pay their doctor's bills, or save enough for their child's college education.\tඋකස් හෝ දොස්තරගේ බිල් ගෙවන්නේ කෙසේද, දරුවන්ගේ විශ්වවිද්‍යාල අධ්‍යාපනයට ප්‍රමාණවත් මුදලක් ඉතිරි කරන්නේ කෙසේදැයි කල්පනා කරමින් දරුවන් නිදාගත් පසු අවදියෙන් සිටින අම්මලා තාත්තලාද සිටිති.\n",
            "A carbon footprint is the amount of carbon dioxide pollution that we produce as a result of our activities. Some people try to reduce their carbon footprint because they are concerned about climate change.\tකාබන් පියසටහනක් යනු අපගේ ක්‍රියාකාරකම්වල ප්‍රතිඵලයක් ලෙස අප විසින් නිපදවන කාබන්ඩයොක්සයිඩ් දූෂණ ප්‍රමාණයයි. සමහර අය දේශගුණික විපර්යාස ගැන සැලකිලිමත් වන නිසා ඔවුන්ගේ කාබන් පියසටහන අඩු කිරීමට උත්සාහ කරති.\n",
            "Since there are usually multiple websites on any given topic, I usually just click the back button when I arrive on any webpage that has pop-up advertising. I just go to the next page found by Google and hope for something less irritating.\tඕනෑම මාතෘකාවක සාමාන්‍යයෙන් වෙබ් පිටු කිහිපයක් ඇති බැවින්, මම සාමාන්‍යයෙන් උත්පතන දැන්වීම් ඇති වෙබ් පිටුවකට පැමිණෙන විට ආපසු බොත්තම ඔබන්නෙමි. මම සරලවම Google විසින් සොයා ගන්නා ලද ඊළඟ පිටුවට ගොස් අඩු කෝපයක් ඇති දෙයක් සොයා ගැනීමට බලාපොරොත්තු වෙමි.\n",
            "If you want to sound like a native speaker, you must be willing to practice saying the same sentence over and over in the same way that banjo players practice the same phrase over and over until they can play it correctly and at the desired tempo.\tඔබට ස්වදේශික කථිකයෙකු ලෙස ශබ්ද කිරීමට අවශ්‍ය නම්, බැන්ජෝ වාදකයෙකු එම වාක්‍ය ඛණ්ඩය නිවැරදිව හා නියමිත වේලාවට වාදනය කරන තෙක් එකම වාක්‍ය ඛණ්ඩය නැවත නැවතත් කීමට පුහුණු වීමට ඔබ කැමති විය යුතුය.\n",
            "It may be impossible to get a completely error-free corpus due to the nature of this kind of collaborative effort. However, if we encourage members to contribute sentences in their own languages rather than experiment in languages they are learning, we might be able to minimize errors.\tමෙම ආකාරයේ සහයෝගී ප්‍රයත්නයේ ස්වභාවය නිසා සම්පූර්ණ දෝෂ රහිත corpus එකක් ලබා ගැනීමට නොහැකි විය හැක. කෙසේ වෙතත්, ඔවුන් ඉගෙන ගන්නා භාෂා සමඟ අත්හදා බැලීම් කරනවාට වඩා ඔවුන්ගේම භාෂාවෙන් වාක්‍ය ඛණ්ඩ දායක කිරීමට අපි සාමාජිකයින් දිරිමත් කරන්නේ නම්, අපට දෝෂ අවම කර ගැනීමට හැකි වනු ඇත.\n",
            "One day, I woke up to find that God had put hair on my face. I shaved it off. The next day, I found that God had put it back on my face, so I shaved it off again. On the third day, when I found that God had put hair back on my face again, I decided to let God have his way. That's why I have a beard.\tදවසක් මම ඇහැරිලා බැලුවා දෙවියන් මගේ මූණට කෙස් ගහලා කියලා. මම ඒක රැවුල කැපුවා. ඊළඟ දවසේ, දෙවියන් වහන්සේ එය මගේ මුහුණට දමා ඇති බව මම දුටුවෙමි, මම එය නැවත රැවුල කපා ගත්තෙමි. තුන්වෙනි දවසේදී, දෙවියන් වහන්සේ නැවතත් මගේ මුහුණට හිසකෙස් දමා ඇති බව මම දුටු විට, මම දෙවියන් වහන්සේට ඔහුගේ මාර්ගයට ඉඩ දීමට තීරණය කළෙමි. ඒකයි මට රැවුල තියෙන්නේ.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SPLIT THE ENGLISH AND SINHALA TRANSLATION PAIRS**"
      ],
      "metadata": {
        "id": "NOCtUrAhR0xY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_pairs = []\n",
        "for line in lines:\n",
        "    english, sinhala = line.split(\"\\t\")\n",
        "    sinhala = \"[start] \" + sinhala + \" [end]\"\n",
        "    text_pairs.append((english, sinhala))\n",
        "for i in range(3):\n",
        "  print(random.choice(text_pairs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VpJUVlesR1eA",
        "outputId": "0884d306-cc0f-4786-f944-ae6168a33aa3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('Tom changed his name.', '[start] ටොමස් ඔහුගේ නම වෙනස් කළේය. [end]')\n",
            "(\"If I were you, I'd buy that one.\", '[start] මම ඔබ නම්, මම එය මිලදී ගන්නෙමි. [end]')\n",
            "('I need you to see this.', '[start] මට ඔයා මේක බලන්න ඕන. [end]')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RANDOMIZE THE DATA**\n"
      ],
      "metadata": {
        "id": "V6YeJOOjR6G5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "random.shuffle(text_pairs)"
      ],
      "metadata": {
        "id": "D-Sc8yYNR6va"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SPITING THE DATASET INTO TRAINING, TESTING AND VALIDATION DATA**"
      ],
      "metadata": {
        "id": "0QQRY6O8SFhi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_val_samples = int(0.15 * len(text_pairs))\n",
        "num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
        "train_pairs = text_pairs[:num_train_samples]\n",
        "val_pairs = text_pairs[num_train_samples:num_train_samples + num_val_samples]\n",
        "test_pairs = text_pairs[num_train_samples + num_val_samples:]\n",
        "print(\"Total sentences:\",len(text_pairs))\n",
        "print(\"Training set size:\",len(train_pairs))\n",
        "print(\"Validation set size:\",len(val_pairs))\n",
        "print(\"Testing set size:\",len(test_pairs))\n",
        "print(\"Total size of the dataset:\",len(train_pairs)+len(val_pairs)+len(test_pairs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p1UFCmsASGDO",
        "outputId": "0af0da7b-b800-42ce-87b0-e00f52480473"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total sentences: 138902\n",
            "Training set size: 97232\n",
            "Validation set size: 20835\n",
            "Testing set size: 20835\n",
            "Total size of the dataset: 138902\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**REMOVING PUNCTUATIONS**"
      ],
      "metadata": {
        "id": "7RKN5WGiSM3r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "strip_chars = string.punctuation + \"¿\"\n",
        "strip_chars = strip_chars.replace(\"[\", \"\")\n",
        "strip_chars = strip_chars.replace(\"]\", \"\")\n",
        "f\"[{re.escape(strip_chars)}]\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "x2DqBIMwSNlX",
        "outputId": "a3d4edc5-635f-4310-b15a-9d89b7444671"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'[!\"\\\\#\\\\$%\\\\&\\'\\\\(\\\\)\\\\*\\\\+,\\\\-\\\\./:;<=>\\\\?@\\\\\\\\\\\\^_`\\\\{\\\\|\\\\}\\\\~¿]'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f\"{3+5}\""
      ],
      "metadata": {
        "id": "cF6lYJEGSQTb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**VECTORIZATION THE ENGLISH AND SINHALA TEXT PAIRS**"
      ],
      "metadata": {
        "id": "Ql67zyqeSWrb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_standardization(input_string):\n",
        "    lowercase = tf.strings.lower(input_string)\n",
        "    return tf.strings.regex_replace(\n",
        "        lowercase, f\"[{re.escape(strip_chars)}]\", \"\")\n",
        "vocab_size = 15000\n",
        "sequence_length = 20\n",
        "source_vectorization = layers.TextVectorization(\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length,\n",
        ")\n",
        "target_vectorization = layers.TextVectorization(\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length + 1,\n",
        "    standardize=custom_standardization,\n",
        ")\n",
        "train_english_texts = [pair[0] for pair in train_pairs]\n",
        "train_sinhala_texts = [pair[1] for pair in train_pairs]"
      ],
      "metadata": {
        "id": "BfTJnpf1SXps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "source_vectorization.adapt(train_english_texts)\n",
        "target_vectorization.adapt(train_sinhala_texts)"
      ],
      "metadata": {
        "id": "l7KYay8JSdsa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PREPARATION OF THE DATASET FRO THE TRANSLATION**"
      ],
      "metadata": {
        "id": "uI1IeD5TShFP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "def format_dataset(eng, sin):\n",
        "    eng = source_vectorization(eng)\n",
        "    sin = target_vectorization(sin)\n",
        "    return ({\n",
        "        \"english\": eng,\n",
        "        \"sinhala\": sin[:, :-1],\n",
        "    }, sin[:, 1:])\n",
        "def make_dataset(pairs):\n",
        "    eng_texts, sin_texts = zip(*pairs)\n",
        "    eng_texts = list(eng_texts)\n",
        "    sin_texts = list(sin_texts)\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, sin_texts))\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.map(format_dataset, num_parallel_calls=4)\n",
        "    return dataset.shuffle(2048).prefetch(16).cache()\n",
        "train_ds = make_dataset(train_pairs)\n",
        "val_ds = make_dataset(val_pairs)\n",
        "for inputs, targets in train_ds.take(1):\n",
        "    print(f\"inputs['english'].shape: {inputs['english'].shape}\")\n",
        "    print(f\"inputs['sinhala'].shape: {inputs['sinhala'].shape}\")\n",
        "    print(f\"targets.shape: {targets.shape}\")\n",
        "inputs['english'].shape: (64, 20)\n",
        "inputs['sinhala'].shape: (64, 20)\n",
        "targets.shape: (64, 20)\n",
        "print(list(train_ds.as_numpy_iterator())[50])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nv6LeGWmSiIF",
        "outputId": "65444591-47d2-4ae8-eaac-7f0cddd991c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs['english'].shape: (64, 20)\n",
            "inputs['sinhala'].shape: (64, 20)\n",
            "targets.shape: (64, 20)\n",
            "({'english': array([[  2, 285, 132, ...,   0,   0,   0],\n",
            "       [ 39,   2, 282, ...,   0,   0,   0],\n",
            "       [335,  30,   7, ...,   0,   0,   0],\n",
            "       ...,\n",
            "       [ 52,  62, 202, ...,   0,   0,   0],\n",
            "       [  3, 333,  10, ...,   0,   0,   0],\n",
            "       [  5, 104,  99, ...,   0,   0,   0]]), 'sinhala': array([[   2,    4,  333, ...,    0,    0,    0],\n",
            "       [   2,    5,   21, ...,    0,    0,    0],\n",
            "       [   2,   20,  295, ...,    0,    0,    0],\n",
            "       ...,\n",
            "       [   2,  283,   27, ...,    0,    0,    0],\n",
            "       [   2,  862, 3968, ...,    0,    0,    0],\n",
            "       [   2,    7,  197, ...,    0,    0,    0]])}, array([[   4,  333,  399, ...,    0,    0,    0],\n",
            "       [   5,   21,  694, ...,    0,    0,    0],\n",
            "       [  20,  295, 7949, ...,    0,    0,    0],\n",
            "       ...,\n",
            "       [ 283,   27,  506, ...,    0,    0,    0],\n",
            "       [ 862, 3968, 1433, ...,    0,    0,    0],\n",
            "       [   7,  197, 8931, ...,    0,    0,    0]]))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TRANSFORMER ENCODER IMPLEMENTED AS A SUBCLASSED LAYER**"
      ],
      "metadata": {
        "id": "hj04V6HdSpXB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(dense_dim, activation=\"relu\"),\n",
        "             layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "    def call(self, inputs, mask=None):\n",
        "        if mask is not None:\n",
        "            mask = mask[:, tf.newaxis, :]\n",
        "        attention_output = self.attention(\n",
        "            inputs, inputs, attention_mask=mask)\n",
        "        proj_input = self.layernorm_1(inputs + attention_output)\n",
        "        proj_output = self.dense_proj(proj_input)\n",
        "        return self.layernorm_2(proj_input + proj_output)\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"num_heads\": self.num_heads,\n",
        "            \"dense_dim\": self.dense_dim,\n",
        "        })\n",
        "        return config"
      ],
      "metadata": {
        "id": "tgehzYhQSqDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**THE TRANSFORMER DECODER**"
      ],
      "metadata": {
        "id": "dI3qhERjSv_I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerDecoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention_1 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.attention_2 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(dense_dim, activation=\"relu\"),\n",
        "             layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "        self.layernorm_3 = layers.LayerNormalization()\n",
        "        self.supports_masking = True\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"num_heads\": self.num_heads,\n",
        "            \"dense_dim\": self.dense_dim,\n",
        "        })\n",
        "        return config\n",
        "    def get_causal_attention_mask(self, inputs):\n",
        "        input_shape = tf.shape(inputs)\n",
        "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
        "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
        "        j = tf.range(sequence_length)\n",
        "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
        "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
        "        mult = tf.concat(\n",
        "            [tf.expand_dims(batch_size, -1),\n",
        "             tf.constant([1, 1], dtype=tf.int32)], axis=0)\n",
        "        return tf.tile(mask, mult)\n",
        "    def call(self, inputs, encoder_outputs, mask=None):\n",
        "        causal_mask = self.get_causal_attention_mask(inputs)\n",
        "        if mask is not None:\n",
        "            padding_mask = tf.cast(\n",
        "                mask[:, tf.newaxis, :], dtype=\"int32\")\n",
        "            padding_mask = tf.minimum(padding_mask, causal_mask)\n",
        "        else:\n",
        "            padding_mask = mask\n",
        "        attention_output_1 = self.attention_1(\n",
        "            query=inputs,\n",
        "            value=inputs,\n",
        "            key=inputs,\n",
        "            attention_mask=causal_mask)\n",
        "        attention_output_1 = self.layernorm_1(inputs + attention_output_1)\n",
        "        attention_output_2 = self.attention_2(\n",
        "            query=attention_output_1,\n",
        "            value=encoder_outputs,\n",
        "            key=encoder_outputs,\n",
        "            attention_mask=padding_mask,\n",
        "        )\n",
        "        attention_output_2 = self.layernorm_2(\n",
        "            attention_output_1 + attention_output_2)\n",
        "        proj_output = self.dense_proj(attention_output_2)\n",
        "        return self.layernorm_3(attention_output_2 + proj_output)\n"
      ],
      "metadata": {
        "id": "E6kbOzXMSw04"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**POSITIONAL ENCODEING**"
      ],
      "metadata": {
        "id": "sCbIW-STS4-X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.token_embeddings = layers.Embedding(\n",
        "            input_dim=input_dim, output_dim=output_dim)\n",
        "        self.position_embeddings = layers.Embedding(\n",
        "            input_dim=sequence_length, output_dim=output_dim)\n",
        "        self.sequence_length = sequence_length\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "    def call(self, inputs):\n",
        "        length = tf.shape(inputs)[-1]\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        embedded_tokens = self.token_embeddings(inputs)\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return embedded_tokens + embedded_positions\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return tf.math.not_equal(inputs, 0)\n",
        "    def get_config(self):\n",
        "        config = super(PositionalEmbedding, self).get_config()\n",
        "        config.update({\n",
        "            \"output_dim\": self.output_dim,\n",
        "            \"sequence_length\": self.sequence_length,\n",
        "            \"input_dim\": self.input_dim,\n",
        "        })\n",
        "        return config"
      ],
      "metadata": {
        "id": "07YgRLAcS5jm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**END-TO-END TRANSFORMER**"
      ],
      "metadata": {
        "id": "OHb6twfdS_DE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embed_dim = 256\n",
        "dense_dim = 2048\n",
        "num_heads = 8\n",
        "encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"english\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n",
        "encoder_outputs = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
        "decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"sinhala\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n",
        "x = TransformerDecoder(embed_dim, dense_dim, num_heads)(x, encoder_outputs)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
        "transformer = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)"
      ],
      "metadata": {
        "id": "fbqLhBh4S_z2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformer.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jFnHYc93TEJT",
        "outputId": "2d9c2a6b-3231-4da0-c03a-eb7ceff6cf4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " english (InputLayer)        [(None, None)]               0         []                            \n",
            "                                                                                                  \n",
            " sinhala (InputLayer)        [(None, None)]               0         []                            \n",
            "                                                                                                  \n",
            " positional_embedding (Posi  (None, None, 256)            3845120   ['english[0][0]']             \n",
            " tionalEmbedding)                                                                                 \n",
            "                                                                                                  \n",
            " positional_embedding_1 (Po  (None, None, 256)            3845120   ['sinhala[0][0]']             \n",
            " sitionalEmbedding)                                                                               \n",
            "                                                                                                  \n",
            " transformer_encoder (Trans  (None, None, 256)            3155456   ['positional_embedding[0][0]']\n",
            " formerEncoder)                                                                                   \n",
            "                                                                                                  \n",
            " transformer_decoder (Trans  (None, None, 256)            5259520   ['positional_embedding_1[0][0]\n",
            " formerDecoder)                                                     ',                            \n",
            "                                                                     'transformer_encoder[0][0]'] \n",
            "                                                                                                  \n",
            " dropout (Dropout)           (None, None, 256)            0         ['transformer_decoder[0][0]'] \n",
            "                                                                                                  \n",
            " dense_4 (Dense)             (None, None, 15000)          3855000   ['dropout[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 19960216 (76.14 MB)\n",
            "Trainable params: 19960216 (76.14 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TRAINING THE TRANSFORMER NEURAL NETWORK**"
      ],
      "metadata": {
        "id": "TdKmNxA9TIa2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transformer.compile(\n",
        "    optimizer=\"rmsprop\",\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"])\n",
        "transformer.fit(train_ds, epochs=40, validation_data=val_ds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DZqRWVMSTJJs",
        "outputId": "a7e44704-a6a0-4647-b175-1fd005a885b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/40\n",
            "1520/1520 [==============================] - 123s 75ms/step - loss: 4.2285 - accuracy: 0.4214 - val_loss: 3.3254 - val_accuracy: 0.5124\n",
            "Epoch 2/40\n",
            "1520/1520 [==============================] - 106s 70ms/step - loss: 3.2785 - accuracy: 0.5176 - val_loss: 2.9444 - val_accuracy: 0.5563\n",
            "Epoch 3/40\n",
            "1520/1520 [==============================] - 116s 76ms/step - loss: 2.9957 - accuracy: 0.5545 - val_loss: 2.8368 - val_accuracy: 0.5759\n",
            "Epoch 4/40\n",
            "1520/1520 [==============================] - 104s 69ms/step - loss: 2.8519 - accuracy: 0.5762 - val_loss: 2.7831 - val_accuracy: 0.5848\n",
            "Epoch 5/40\n",
            "1520/1520 [==============================] - 104s 69ms/step - loss: 2.7627 - accuracy: 0.5923 - val_loss: 2.7631 - val_accuracy: 0.5935\n",
            "Epoch 6/40\n",
            "1520/1520 [==============================] - 104s 68ms/step - loss: 2.6996 - accuracy: 0.6037 - val_loss: 2.7403 - val_accuracy: 0.5978\n",
            "Epoch 7/40\n",
            "1520/1520 [==============================] - 104s 69ms/step - loss: 2.6391 - accuracy: 0.6151 - val_loss: 2.7402 - val_accuracy: 0.6003\n",
            "Epoch 8/40\n",
            "1520/1520 [==============================] - 104s 68ms/step - loss: 2.5893 - accuracy: 0.6233 - val_loss: 2.7243 - val_accuracy: 0.6055\n",
            "Epoch 9/40\n",
            "1520/1520 [==============================] - 104s 69ms/step - loss: 2.5407 - accuracy: 0.6313 - val_loss: 2.7229 - val_accuracy: 0.6059\n",
            "Epoch 10/40\n",
            "1520/1520 [==============================] - 104s 68ms/step - loss: 2.4997 - accuracy: 0.6385 - val_loss: 2.7326 - val_accuracy: 0.6065\n",
            "Epoch 11/40\n",
            "1520/1520 [==============================] - 104s 68ms/step - loss: 2.4621 - accuracy: 0.6444 - val_loss: 2.7263 - val_accuracy: 0.6119\n",
            "Epoch 12/40\n",
            "1520/1520 [==============================] - 104s 68ms/step - loss: 2.4279 - accuracy: 0.6505 - val_loss: 2.7356 - val_accuracy: 0.6105\n",
            "Epoch 13/40\n",
            "1520/1520 [==============================] - 104s 68ms/step - loss: 2.3954 - accuracy: 0.6559 - val_loss: 2.7531 - val_accuracy: 0.6118\n",
            "Epoch 14/40\n",
            "1520/1520 [==============================] - 104s 68ms/step - loss: 2.3700 - accuracy: 0.6602 - val_loss: 2.7326 - val_accuracy: 0.6165\n",
            "Epoch 15/40\n",
            "1520/1520 [==============================] - 104s 68ms/step - loss: 2.3461 - accuracy: 0.6646 - val_loss: 2.7560 - val_accuracy: 0.6149\n",
            "Epoch 16/40\n",
            "1520/1520 [==============================] - 104s 68ms/step - loss: 2.3205 - accuracy: 0.6686 - val_loss: 2.7635 - val_accuracy: 0.6177\n",
            "Epoch 17/40\n",
            "1520/1520 [==============================] - 104s 68ms/step - loss: 2.2977 - accuracy: 0.6723 - val_loss: 2.7646 - val_accuracy: 0.6177\n",
            "Epoch 18/40\n",
            "1520/1520 [==============================] - 104s 68ms/step - loss: 2.2718 - accuracy: 0.6768 - val_loss: 2.7584 - val_accuracy: 0.6184\n",
            "Epoch 19/40\n",
            "1520/1520 [==============================] - 104s 68ms/step - loss: 2.2446 - accuracy: 0.6819 - val_loss: 2.7718 - val_accuracy: 0.6196\n",
            "Epoch 20/40\n",
            "1520/1520 [==============================] - 104s 68ms/step - loss: 2.2208 - accuracy: 0.6855 - val_loss: 2.7662 - val_accuracy: 0.6222\n",
            "Epoch 21/40\n",
            "1520/1520 [==============================] - 104s 69ms/step - loss: 2.1979 - accuracy: 0.6891 - val_loss: 2.7729 - val_accuracy: 0.6217\n",
            "Epoch 22/40\n",
            "1520/1520 [==============================] - 104s 69ms/step - loss: 2.1742 - accuracy: 0.6931 - val_loss: 2.7963 - val_accuracy: 0.6215\n",
            "Epoch 23/40\n",
            "1520/1520 [==============================] - 104s 68ms/step - loss: 2.1510 - accuracy: 0.6967 - val_loss: 2.8060 - val_accuracy: 0.6219\n",
            "Epoch 24/40\n",
            "1520/1520 [==============================] - 104s 68ms/step - loss: 2.1297 - accuracy: 0.7005 - val_loss: 2.8270 - val_accuracy: 0.6243\n",
            "Epoch 25/40\n",
            "1520/1520 [==============================] - 104s 68ms/step - loss: 2.1073 - accuracy: 0.7043 - val_loss: 2.8009 - val_accuracy: 0.6260\n",
            "Epoch 26/40\n",
            "1520/1520 [==============================] - 113s 75ms/step - loss: 2.0876 - accuracy: 0.7072 - val_loss: 2.8482 - val_accuracy: 0.6253\n",
            "Epoch 27/40\n",
            "1520/1520 [==============================] - 104s 68ms/step - loss: 2.0646 - accuracy: 0.7112 - val_loss: 2.8612 - val_accuracy: 0.6258\n",
            "Epoch 28/40\n",
            "1520/1520 [==============================] - 114s 75ms/step - loss: 2.0448 - accuracy: 0.7143 - val_loss: 2.8546 - val_accuracy: 0.6265\n",
            "Epoch 29/40\n",
            "1520/1520 [==============================] - 104s 68ms/step - loss: 2.0269 - accuracy: 0.7177 - val_loss: 2.8896 - val_accuracy: 0.6262\n",
            "Epoch 30/40\n",
            "1520/1520 [==============================] - 104s 68ms/step - loss: 2.0065 - accuracy: 0.7208 - val_loss: 2.8778 - val_accuracy: 0.6271\n",
            "Epoch 31/40\n",
            "1520/1520 [==============================] - 104s 68ms/step - loss: 1.9885 - accuracy: 0.7236 - val_loss: 2.9177 - val_accuracy: 0.6242\n",
            "Epoch 32/40\n",
            "1520/1520 [==============================] - 104s 68ms/step - loss: 1.9743 - accuracy: 0.7261 - val_loss: 2.9141 - val_accuracy: 0.6272\n",
            "Epoch 33/40\n",
            "1520/1520 [==============================] - 104s 68ms/step - loss: 1.9554 - accuracy: 0.7290 - val_loss: 2.9124 - val_accuracy: 0.6288\n",
            "Epoch 34/40\n",
            "1520/1520 [==============================] - 113s 75ms/step - loss: 1.9371 - accuracy: 0.7325 - val_loss: 2.9568 - val_accuracy: 0.6288\n",
            "Epoch 35/40\n",
            "1520/1520 [==============================] - 113s 75ms/step - loss: 1.9188 - accuracy: 0.7352 - val_loss: 2.9487 - val_accuracy: 0.6270\n",
            "Epoch 36/40\n",
            "1520/1520 [==============================] - 114s 75ms/step - loss: 1.9034 - accuracy: 0.7375 - val_loss: 2.9676 - val_accuracy: 0.6314\n",
            "Epoch 37/40\n",
            "1520/1520 [==============================] - 104s 68ms/step - loss: 1.8901 - accuracy: 0.7399 - val_loss: 2.9883 - val_accuracy: 0.6306\n",
            "Epoch 38/40\n",
            "1520/1520 [==============================] - 104s 68ms/step - loss: 1.8736 - accuracy: 0.7431 - val_loss: 2.9812 - val_accuracy: 0.6321\n",
            "Epoch 39/40\n",
            "1520/1520 [==============================] - 104s 69ms/step - loss: 1.8577 - accuracy: 0.7452 - val_loss: 3.0006 - val_accuracy: 0.6327\n",
            "Epoch 40/40\n",
            "1520/1520 [==============================] - 104s 68ms/step - loss: 1.8408 - accuracy: 0.7482 - val_loss: 3.0127 - val_accuracy: 0.6344\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x789548371900>"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MANUAL TESTING OF THE TRANSLATION MODEL WITH 20 NEW SENTENCES**"
      ],
      "metadata": {
        "id": "Mgm9Ugt6TXPJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "sin_vocab = target_vectorization.get_vocabulary()\n",
        "sin_index_lookup = dict(zip(range(len(sin_vocab)), sin_vocab))\n",
        "max_decoded_sentence_length = 20\n",
        "\n",
        "def decode_sequence(input_sentence):\n",
        "    tokenized_input_sentence = source_vectorization([input_sentence])\n",
        "    decoded_sentence = \"[start]\"\n",
        "    for i in range(max_decoded_sentence_length):\n",
        "        tokenized_target_sentence = target_vectorization(\n",
        "            [decoded_sentence])[:, :-1]\n",
        "        predictions = transformer(\n",
        "            [tokenized_input_sentence, tokenized_target_sentence])\n",
        "        sampled_token_index = np.argmax(predictions[0, i, :])\n",
        "        sampled_token = sin_index_lookup[sampled_token_index]\n",
        "        decoded_sentence += \" \" + sampled_token\n",
        "        if sampled_token == \"[end]\":\n",
        "            break\n",
        "    return decoded_sentence\n",
        "\n",
        "test_eng_texts = [pair[0] for pair in test_pairs]\n",
        "for _ in range(20):\n",
        "    input_sentence = random.choice(test_eng_texts)\n",
        "    print(\"-\")\n",
        "    print(input_sentence)\n",
        "    print(decode_sequence(input_sentence))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wRaIJE_qTZqL",
        "outputId": "632a0da3-80b8-4ed1-a166-47c66a3a8408"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-\n",
            "I'm always busy with my homework.\n",
            "[start] මම නිතරම මගේ ගෙදර වැඩ සමඟ කාර්යබහුලයි [end]\n",
            "-\n",
            "Are you going to come visit me?\n",
            "[start] ඔයා මාව බලන්න එයි [end]\n",
            "-\n",
            "I didn't make anything to eat.\n",
            "[start] මම කිසිම දෙයක් කන්න ඕන වුණේ නැහැ [end]\n",
            "-\n",
            "Tom has made an about-face.\n",
            "[start] ටොම්ට [UNK] සාදා ඇත [end]\n",
            "-\n",
            "When he was a child, his ambition was to be an English teacher.\n",
            "[start] ඔහුගේ පුතා ගුරුවරියක් වූයේ ඔහු ඉංග්‍රීසි පමණක් [UNK] [end]\n",
            "-\n",
            "I received a letter written in English yesterday.\n",
            "[start] ඊයේ මම ලිපියක් භාෂාවෙන් ලිපියක් ලියා තිබුණා [end]\n",
            "-\n",
            "I saw the car hit a man.\n",
            "[start] මම දැක්කා කාර් එක කොල්ලෙක් [end]\n",
            "-\n",
            "I've finished translating everything you asked me to translate.\n",
            "[start] ඔබ මගෙන් පරිවර්තනය කළ සෑම දෙයක්ම මම ඉල්ලා අවසන් කරන බව මම මකා දමා ඇත [end]\n",
            "-\n",
            "He abandoned his wife and children.\n",
            "[start] ඔහු තම බිරිඳ සහ දරුවන් අතහැර ගියේය [end]\n",
            "-\n",
            "Study these sentences.\n",
            "[start] මෙම [UNK] ඉගෙන ගන්න [end]\n",
            "-\n",
            "I'm tied up right now.\n",
            "[start] මම දැන් මගේ ගෑස් කැඩුවා [end]\n",
            "-\n",
            "Tom sent me a message.\n",
            "[start] ටොම් මට පණිවිඩයක් දැම්මා [end]\n",
            "-\n",
            "It's my fault that we got here late.\n",
            "[start] අපි පරක්කු නොවන එක තමයි අපි ඔබ මෙහි [end]\n",
            "-\n",
            "He changed his mind daily.\n",
            "[start] ඔහු තම ඒ [UNK] වෙනස් කළේය [end]\n",
            "-\n",
            "I don't understand your question.\n",
            "[start] මට ඔබේ ප්‍රශ්නය තේරෙන්නේ නැහැ [end]\n",
            "-\n",
            "Do you play golf?\n",
            "[start] ඔබ ගොල්ෆ් ක්‍රීඩා කරනවාද [end]\n",
            "-\n",
            "He sometimes goes to Tokyo on business.\n",
            "[start] සමහර අය අනාගතය ගැන ඔබ විදේශගත වෙනවා [end]\n",
            "-\n",
            "We can help.\n",
            "[start] අපිට උදව් කරන්න පුළුවන් [end]\n",
            "-\n",
            "You're not the only Canadian here.\n",
            "[start] මෙහි කැනේඩියානු ජාතිකයෙක් නොවේ [end]\n",
            "-\n",
            "The king governed the country.\n",
            "[start] සොල්දාදුවන් රටේ කාන්තාවන් [UNK] [end]\n"
          ]
        }
      ]
    }
  ]
}